{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a1ff403",
   "metadata": {},
   "source": [
    "## Read the saved model and use the new data set (not exposed to training) \n",
    "## And makes the prediction\n",
    "## Model trained on pion (+), HCAL, 17 deg, discrete energy\n",
    "## FUNCTIONS NEEDED\n",
    "* generator.py\n",
    "* deepsets_test_prediction.py\n",
    "* configs/default.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8325a7fb",
   "metadata": {},
   "source": [
    "## Instructions how to run the codes \n",
    "\n",
    "* You can run the model on continuous or discrete data set remember they may have different mean and std\n",
    "* I would recommend to copy \"means.p\" and \"std.p\" files from respective directory to your \"output_dir\"\n",
    "* From output directory 'generator.py\" script which produces test dataset will read means and stds \n",
    "* For continuous energy means.p and std.p are in directory \"/media/miguel/Elements/Data_hcali/Data1/log10_Uniform_03-23/DeepSets_output/preprocessed_data\"\n",
    "* For discrete energy mean.p and std.p are in directory \"/media/miguel/Elements/Data_hcali/Data1/log10_Uniform_03-23/DeepSets_output/preprocessed_data_discrete_pi+_17deg_all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e760bc6",
   "metadata": {},
   "source": [
    "## Not necessary to change anything in cell below\n",
    "## May be some WARNING from tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd3bc9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 16:42:37.007596: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-06 16:42:37.561844: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/bishnu/Library/root/lib:/usr/local/lib:/usr/local/lib:/usr/local/cuda-11.8/lib64\n",
      "2023-04-06 16:42:37.561905: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/bishnu/Library/root/lib:/usr/local/lib:/usr/local/lib:/usr/local/cuda-11.8/lib64\n",
      "2023-04-06 16:42:37.561910: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from deepsets_test_prediction import *\n",
    "\n",
    "data_config = config['data']\n",
    "model_config = config['model']\n",
    "train_config = config['training']\n",
    "\n",
    "#data_dir = data_config['data_dir']                                                                                                                                                         \n",
    "\n",
    "#num_val_files = data_config['num_val_files']                                                                                                                                               \n",
    "batch_size = data_config['batch_size']\n",
    "shuffle = data_config['shuffle']\n",
    "num_procs = 2\n",
    "preprocess = data_config['preprocess']\n",
    "\n",
    "already_preprocessed = False\n",
    "calc_stats = False\n",
    "concat_input = model_config['concat_input']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dc8433",
   "metadata": {},
   "source": [
    "## You can play with these parameters\n",
    "* CHOOSE THE NUMBER OF FILES FOR THE TEST DATA \n",
    "* CHOSE THE DATA 'discrete' OR 'continuous'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ccc9aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHOOSE HOW MANY FILES YOU WANT TO USE FOR THE TEST DATA SETS\n",
    "num_test_files =1\n",
    "\n",
    "## CHOOSE THE DISCRETE OR CONTINUOUS DATA SET\n",
    "data_type='discrete'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4660513",
   "metadata": {},
   "source": [
    "## CHOOSE THE MODEL DIRECTORY\n",
    "* path_to_model can remain same (You can read the model from my directory or copy to your place\n",
    "* path to data can also remain same \n",
    "* BUT OUTPUT_DIR NEEDS TO BE AT YOUR PLACE, YOU NEED TO COPY RIGHT MEAN AND STD FILES FROM ABOVE MENTIONED DIRECTORIES TO HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32a233a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PATH TO THE DATA AND MODEL DIRECTORY                                                                                                                                                     \n",
    "path_to_model='/media/miguel/Elements/Data_hcali/Data1/log10_Uniform_03-23/DeepSets_output/\\\n",
    "    results_discrete_pi+_17deg_all/Block_20230331_1758_concatTrue'\n",
    "\n",
    "## TEST DATA SET DIRECTORU\n",
    "if data_type=='continuous':\n",
    "    data_dir=\"/media/miguel/Elements/Data_hcali/Data1/log10_Uniform_03-23/log10_pi+_discrete_17deg_20_20k/\"\n",
    "\n",
    "## FOR CONTINUOUS DATA SET\n",
    "elif data_type=='discrete':\n",
    "    data_dir=\"/media/miguel/Elements/Data_hcali/Data1/log10_Uniform_03-23/log10_pi+_Uniform_0-140Gev_17deg_testdata/\"\n",
    "else:\n",
    "    print(\"What is the data type you want to pick\")\n",
    "\n",
    "### WHERE YOU WANT TO PUT THE PREDICTION USING MODEL AND TEST DATA SETS\n",
    "## This directory should contain the mean and standard deviation\n",
    "output_dir ='/home/bishnu/EIC/deepsets_output/test_test'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27d61c4",
   "metadata": {},
   "source": [
    "## READ MODEL \n",
    "## NO ANY CHANGES NEEDED IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cfa0d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if preprocess:\n",
    "        train_output_dir = output_dir + '/train/'\n",
    "        val_output_dir = output_dir + '/test/'\n",
    "\n",
    "### MODEL READ                                                                                                                                                                              \n",
    "model = models.BlockModel(global_output_size=1, model_config=model_config)\n",
    "checkpoint = tf.train.Checkpoint(module=model)\n",
    "\n",
    "last_ckpt_path = path_to_model + '/last_saved_model'\n",
    "\n",
    "if os.path.exists(last_ckpt_path+'.index'):\n",
    "        checkpoint.read(last_ckpt_path)\n",
    "        \n",
    "        \n",
    "root_files = np.sort(glob.glob(data_dir+'*root'))\n",
    "test_start = 0\n",
    "test_end = test_start + num_test_files\n",
    "#val_end = train_end + num_val_files                                                                                                                                                        \n",
    "root_test_files = root_files[test_start:test_end]        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3ef7b2",
   "metadata": {},
   "source": [
    "## GENERATES THE TEST DATA SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33918aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing and saving data to /home/bishnu/EIC/deepsets_output/test_test/test\n",
      "Processing file number 0\n",
      "Finished processing file number 0\n"
     ]
    }
   ],
   "source": [
    "data_gen_val = MPGraphDataGenerator(file_list=root_test_files,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=shuffle,\n",
    "                                        num_procs=num_procs,\n",
    "                                        calc_stats=calc_stats,\n",
    "                                        is_val=True,\n",
    "                                        preprocess=preprocess,\n",
    "                                        already_preprocessed=already_preprocessed,\n",
    "                                        output_dir=val_output_dir)\n",
    "                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52a9fb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 16:43:00.085428: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-06 16:43:00.085763: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-06 16:43:00.105334: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/bishnu/Library/root/lib:/usr/local/lib:/usr/local/lib:/usr/local/cuda-11.8/lib64\n",
      "2023-04-06 16:43:00.105361: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-04-06 16:43:00.105894: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "samp_graph, samp_target = next(get_batch(data_gen_val.generator()))\n",
    "data_gen_val.kill_procs()\n",
    "graph_spec = utils_tf.specs_from_graphs_tuple(samp_graph, True, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff88684",
   "metadata": {},
   "source": [
    "## MAKES THE MODEL PREDICTION AND SAVES THE PREDICTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c287266e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO AI M DONE\n"
     ]
    }
   ],
   "source": [
    "all_targets = []\n",
    "all_outputs = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for graph_data_val, targets_val in get_batch(data_gen_val.generator()):#val_iter):                                                                                                          \n",
    "        #losses_val, output_vals = val_step(graph_data_val, targets_val)                                                                                                                    \n",
    "        #print('i -s ********   ', i)                                                                                                                                                       \n",
    "        output_vals = val_step(graph_data_val, targets_val)\n",
    "        \n",
    "        targets_val = targets_val.numpy()\n",
    "        output_vals = output_vals.numpy().squeeze()\n",
    "\n",
    "        all_targets.append(targets_val)\n",
    "        all_outputs.append(output_vals)\n",
    "        \n",
    "all_targets = np.concatenate(all_targets)\n",
    "all_outputs = np.concatenate(all_outputs)\n",
    "\n",
    "\n",
    "np.savez(output_dir+'/predictions_appended', targets=all_targets, outputs=all_outputs)\n",
    "\n",
    "print(\"HELLO I  AM DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb02e2db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2b149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "use_tensorflow",
   "language": "python",
   "name": "use_tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
